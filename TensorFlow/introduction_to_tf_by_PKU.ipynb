{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([1.0, 2.0])\n",
    "b = tf.constant([3.0, 4.0])\n",
    "result = a + b\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1.0, 2.0]])\n",
    "w = tf.constant([[3.0], [4.0]])\n",
    "y = tf.matmul(x, w)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(\"y is :\\n\", sess.run(y, feed_dict={x:[[0.7, 0.5], \n",
    "                                                [0.2, 0.3],\n",
    "                                                [0.3, 0.4],\n",
    "                                                [0.4, 0.5]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(1, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(\"y is :\\n\", sess.run(y, feed_dict={x:[[0.7, 0.5]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "\n",
    "#基于seed产生随机数\n",
    "rdm = np.random.RandomState(SEED)\n",
    "#随机数返回32行2列的矩阵 表示32组 体积和重量 作为输入数据集\n",
    "X = rdm.rand(32,2)\n",
    "#从X这个32行2列的矩阵中 取出一行 判断如果和小于1 给Y赋值1 如果和不小于1 给Y赋值0 \n",
    "#作为输入数据集的标签（正确答案） \n",
    "Y_ = [[int(x0 + x1 < 1)] for (x0, x1) in X]\n",
    "print (\"X:\\n\",X)\n",
    "print (\"Y_:\\n\",Y_)\n",
    "\n",
    "#1定义神经网络的输入、参数和输出,定义前向传播过程。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_= tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "w1= tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2= tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "#2定义损失函数及反向传播方法。\n",
    "loss_mse = tf.reduce_mean(tf.square(y-y_)) \n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "#train_step = tf.train.MomentumOptimizer(0.001,0.9).minimize(loss_mse)\n",
    "#train_step = tf.train.AdamOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "#3生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    # 输出目前（未经训练）的参数取值。\n",
    "    print (\"w1:\\n\", sess.run(w1))\n",
    "    print (\"w2:\\n\", sess.run(w2))\n",
    "    print (\"\\n\")\n",
    "    \n",
    "    # 训练模型。\n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss_mse, feed_dict={x: X, y_: Y_})\n",
    "            print(\"After %d training step(s), loss_mse on all data is %g\" % (i, total_loss))\n",
    "    \n",
    "    # 输出训练后的参数取值。\n",
    "    print (\"\\n\")\n",
    "    print (\"w1:\\n\", sess.run(w1))\n",
    "    print (\"w2:\\n\", sess.run(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "seed = 23455\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "X = rng.rand(32, 2)\n",
    "Y = [[int(x0 + x1 < 1) for (x0, x1) in X]]\n",
    "print(\"X:\\n\", X)\n",
    "print(\"Y:\\n\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.80974597]\n",
      " [ 1.4852903 ]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[-0.46074435]\n",
      " [ 1.641878  ]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[-0.21939856]\n",
      " [ 1.6984766 ]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[-0.04415595]\n",
      " [ 1.7003176 ]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[0.08942621]\n",
      " [1.673328  ]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[0.19583553]\n",
      " [1.6322677 ]] \n",
      "\n",
      "After 3000 training steps, w1 is: \n",
      "[[0.28375748]\n",
      " [1.5854434 ]] \n",
      "\n",
      "After 3500 training steps, w1 is: \n",
      "[[0.35848638]\n",
      " [1.5374471 ]] \n",
      "\n",
      "After 4000 training steps, w1 is: \n",
      "[[0.4233252]\n",
      " [1.4907392]] \n",
      "\n",
      "After 4500 training steps, w1 is: \n",
      "[[0.48040032]\n",
      " [1.4465573 ]] \n",
      "\n",
      "After 5000 training steps, w1 is: \n",
      "[[0.5311361]\n",
      " [1.4054534]] \n",
      "\n",
      "After 5500 training steps, w1 is: \n",
      "[[0.57653254]\n",
      " [1.367594  ]] \n",
      "\n",
      "After 6000 training steps, w1 is: \n",
      "[[0.6173259]\n",
      " [1.3329402]] \n",
      "\n",
      "After 6500 training steps, w1 is: \n",
      "[[0.65408474]\n",
      " [1.3013425 ]] \n",
      "\n",
      "After 7000 training steps, w1 is: \n",
      "[[0.68726856]\n",
      " [1.2726018 ]] \n",
      "\n",
      "After 7500 training steps, w1 is: \n",
      "[[0.7172598]\n",
      " [1.2465004]] \n",
      "\n",
      "After 8000 training steps, w1 is: \n",
      "[[0.74438614]\n",
      " [1.2228196 ]] \n",
      "\n",
      "After 8500 training steps, w1 is: \n",
      "[[0.7689325]\n",
      " [1.2013482]] \n",
      "\n",
      "After 9000 training steps, w1 is: \n",
      "[[0.79115146]\n",
      " [1.1818888 ]] \n",
      "\n",
      "After 9500 training steps, w1 is: \n",
      "[[0.81126714]\n",
      " [1.1642567 ]] \n",
      "\n",
      "After 10000 training steps, w1 is: \n",
      "[[0.8294814]\n",
      " [1.1482829]] \n",
      "\n",
      "After 10500 training steps, w1 is: \n",
      "[[0.84597576]\n",
      " [1.1338127 ]] \n",
      "\n",
      "After 11000 training steps, w1 is: \n",
      "[[0.8609128]\n",
      " [1.1207061]] \n",
      "\n",
      "After 11500 training steps, w1 is: \n",
      "[[0.87444043]\n",
      " [1.1088346 ]] \n",
      "\n",
      "After 12000 training steps, w1 is: \n",
      "[[0.88669145]\n",
      " [1.0980824 ]] \n",
      "\n",
      "After 12500 training steps, w1 is: \n",
      "[[0.8977863]\n",
      " [1.0883439]] \n",
      "\n",
      "After 13000 training steps, w1 is: \n",
      "[[0.9078348]\n",
      " [1.0795243]] \n",
      "\n",
      "After 13500 training steps, w1 is: \n",
      "[[0.91693527]\n",
      " [1.0715363 ]] \n",
      "\n",
      "After 14000 training steps, w1 is: \n",
      "[[0.92517716]\n",
      " [1.0643018 ]] \n",
      "\n",
      "After 14500 training steps, w1 is: \n",
      "[[0.93264157]\n",
      " [1.0577497 ]] \n",
      "\n",
      "After 15000 training steps, w1 is: \n",
      "[[0.9394023]\n",
      " [1.0518153]] \n",
      "\n",
      "After 15500 training steps, w1 is: \n",
      "[[0.9455251]\n",
      " [1.0464406]] \n",
      "\n",
      "After 16000 training steps, w1 is: \n",
      "[[0.95107025]\n",
      " [1.0415728 ]] \n",
      "\n",
      "After 16500 training steps, w1 is: \n",
      "[[0.9560928]\n",
      " [1.037164 ]] \n",
      "\n",
      "After 17000 training steps, w1 is: \n",
      "[[0.96064115]\n",
      " [1.0331714 ]] \n",
      "\n",
      "After 17500 training steps, w1 is: \n",
      "[[0.96476096]\n",
      " [1.0295546 ]] \n",
      "\n",
      "After 18000 training steps, w1 is: \n",
      "[[0.9684917]\n",
      " [1.0262802]] \n",
      "\n",
      "After 18500 training steps, w1 is: \n",
      "[[0.9718707]\n",
      " [1.0233142]] \n",
      "\n",
      "After 19000 training steps, w1 is: \n",
      "[[0.974931 ]\n",
      " [1.0206276]] \n",
      "\n",
      "After 19500 training steps, w1 is: \n",
      "[[0.9777026]\n",
      " [1.0181949]] \n",
      "\n",
      "Final w1 is: \n",
      " [[0.98019385]\n",
      " [1.0159807 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32, 2)\n",
    "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1 = tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    Steps = 20000\n",
    "    for i in range(Steps):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start: end], y_: Y_[start: end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After {} training steps, w1 is: \".format (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training steps, w1 is: \n",
      "[[-0.762993 ]\n",
      " [ 1.5095658]] \n",
      "\n",
      "After 500 training steps, w1 is: \n",
      "[[1.0235443]\n",
      " [1.0463371]] \n",
      "\n",
      "After 1000 training steps, w1 is: \n",
      "[[1.0174844]\n",
      " [1.0406414]] \n",
      "\n",
      "After 1500 training steps, w1 is: \n",
      "[[1.0211805]\n",
      " [1.0472372]] \n",
      "\n",
      "After 2000 training steps, w1 is: \n",
      "[[1.0179386]\n",
      " [1.041272 ]] \n",
      "\n",
      "After 2500 training steps, w1 is: \n",
      "[[1.0205938]\n",
      " [1.0390443]] \n",
      "\n",
      "After 3000 training steps, w1 is: \n",
      "[[1.0242898]\n",
      " [1.04564  ]] \n",
      "\n",
      "After 3500 training steps, w1 is: \n",
      "[[1.01823  ]\n",
      " [1.0399443]] \n",
      "\n",
      "After 4000 training steps, w1 is: \n",
      "[[1.021926]\n",
      " [1.04654 ]] \n",
      "\n",
      "After 4500 training steps, w1 is: \n",
      "[[1.0245812]\n",
      " [1.0443122]] \n",
      "\n",
      "After 5000 training steps, w1 is: \n",
      "[[1.0195622]\n",
      " [1.04744  ]] \n",
      "\n",
      "After 5500 training steps, w1 is: \n",
      "[[1.0222174]\n",
      " [1.0452123]] \n",
      "\n",
      "After 6000 training steps, w1 is: \n",
      "[[1.0161575]\n",
      " [1.0395166]] \n",
      "\n",
      "After 6500 training steps, w1 is: \n",
      "[[1.0198536]\n",
      " [1.0461123]] \n",
      "\n",
      "After 7000 training steps, w1 is: \n",
      "[[1.0169773]\n",
      " [1.0424668]] \n",
      "\n",
      "After 7500 training steps, w1 is: \n",
      "[[1.0174898]\n",
      " [1.0470123]] \n",
      "\n",
      "After 8000 training steps, w1 is: \n",
      "[[1.0233285]\n",
      " [1.0468347]] \n",
      "\n",
      "After 8500 training steps, w1 is: \n",
      "[[1.0172687]\n",
      " [1.041139 ]] \n",
      "\n",
      "After 9000 training steps, w1 is: \n",
      "[[1.0199238]\n",
      " [1.0389112]] \n",
      "\n",
      "After 9500 training steps, w1 is: \n",
      "[[1.0236199]\n",
      " [1.045507 ]] \n",
      "\n",
      "After 10000 training steps, w1 is: \n",
      "[[1.01756  ]\n",
      " [1.0398113]] \n",
      "\n",
      "After 10500 training steps, w1 is: \n",
      "[[1.0212561]\n",
      " [1.046407 ]] \n",
      "\n",
      "After 11000 training steps, w1 is: \n",
      "[[1.0239112]\n",
      " [1.0441792]] \n",
      "\n",
      "After 11500 training steps, w1 is: \n",
      "[[1.0188923]\n",
      " [1.047307 ]] \n",
      "\n",
      "After 12000 training steps, w1 is: \n",
      "[[1.0238953]\n",
      " [1.0444468]] \n",
      "\n",
      "After 12500 training steps, w1 is: \n",
      "[[1.0178354]\n",
      " [1.0387511]] \n",
      "\n",
      "After 13000 training steps, w1 is: \n",
      "[[1.0215315]\n",
      " [1.0453469]] \n",
      "\n",
      "After 13500 training steps, w1 is: \n",
      "[[1.0154716]\n",
      " [1.0396512]] \n",
      "\n",
      "After 14000 training steps, w1 is: \n",
      "[[1.0191677]\n",
      " [1.0462469]] \n",
      "\n",
      "After 14500 training steps, w1 is: \n",
      "[[1.0162914]\n",
      " [1.0426013]] \n",
      "\n",
      "After 15000 training steps, w1 is: \n",
      "[[1.0189465]\n",
      " [1.0403736]] \n",
      "\n",
      "After 15500 training steps, w1 is: \n",
      "[[1.0226426]\n",
      " [1.0469693]] \n",
      "\n",
      "After 16000 training steps, w1 is: \n",
      "[[1.0252978]\n",
      " [1.0447415]] \n",
      "\n",
      "After 16500 training steps, w1 is: \n",
      "[[1.0192379]\n",
      " [1.0390458]] \n",
      "\n",
      "After 17000 training steps, w1 is: \n",
      "[[1.022934 ]\n",
      " [1.0456415]] \n",
      "\n",
      "After 17500 training steps, w1 is: \n",
      "[[1.0168741]\n",
      " [1.0399458]] \n",
      "\n",
      "After 18000 training steps, w1 is: \n",
      "[[1.0205702]\n",
      " [1.0465416]] \n",
      "\n",
      "After 18500 training steps, w1 is: \n",
      "[[1.0232253]\n",
      " [1.0443138]] \n",
      "\n",
      "After 19000 training steps, w1 is: \n",
      "[[1.0182064]\n",
      " [1.0474416]] \n",
      "\n",
      "After 19500 training steps, w1 is: \n",
      "[[1.0232093]\n",
      " [1.0445814]] \n",
      "\n",
      "Final w1 is: \n",
      " [[1.0225189]\n",
      " [1.0416598]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "SEED = 23455\n",
    "COST = 1\n",
    "PROFIT = 9\n",
    "\n",
    "rdm = np.random.RandomState(SEED)\n",
    "X = rdm.rand(32, 2)\n",
    "Y = [[x1 + x2 + rdm.rand()/10.0-0.05] for (x1, x2) in X]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "w1 = tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * COST, (y_ - y) * PROFIT))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    Steps = 20000\n",
    "    for i in range(Steps):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start: end], y_: Y_[start: end]})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After {} training steps, w1 is: \".format (i))\n",
    "            print(sess.run(w1), \"\\n\")\n",
    "    print(\"Final w1 is: \\n\", sess.run(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 steps: w is 4.998799800872803,    loss is 35.985599517822266\n",
      "After 1 steps: w is 4.997600078582764,    loss is 35.97120666503906\n",
      "After 2 steps: w is 4.996400356292725,    loss is 35.956817626953125\n",
      "After 3 steps: w is 4.995201110839844,    loss is 35.94243621826172\n",
      "After 4 steps: w is 4.994001865386963,    loss is 35.92805862426758\n",
      "After 5 steps: w is 4.99280309677124,    loss is 35.91368865966797\n",
      "After 6 steps: w is 4.991604328155518,    loss is 35.899322509765625\n",
      "After 7 steps: w is 4.990406036376953,    loss is 35.88496398925781\n",
      "After 8 steps: w is 4.989207744598389,    loss is 35.870609283447266\n",
      "After 9 steps: w is 4.988009929656982,    loss is 35.85626220703125\n",
      "After 10 steps: w is 4.986812114715576,    loss is 35.8419189453125\n",
      "After 11 steps: w is 4.985614776611328,    loss is 35.82758331298828\n",
      "After 12 steps: w is 4.98441743850708,    loss is 35.81325149536133\n",
      "After 13 steps: w is 4.98322057723999,    loss is 35.798927307128906\n",
      "After 14 steps: w is 4.9820237159729,    loss is 35.78460693359375\n",
      "After 15 steps: w is 4.980827331542969,    loss is 35.770294189453125\n",
      "After 16 steps: w is 4.979630947113037,    loss is 35.755985260009766\n",
      "After 17 steps: w is 4.978435039520264,    loss is 35.74168395996094\n",
      "After 18 steps: w is 4.97723913192749,    loss is 35.727386474609375\n",
      "After 19 steps: w is 4.976043701171875,    loss is 35.713096618652344\n",
      "After 20 steps: w is 4.97484827041626,    loss is 35.69881057739258\n",
      "After 21 steps: w is 4.973653316497803,    loss is 35.684532165527344\n",
      "After 22 steps: w is 4.972458362579346,    loss is 35.670257568359375\n",
      "After 23 steps: w is 4.971263885498047,    loss is 35.65599060058594\n",
      "After 24 steps: w is 4.970069408416748,    loss is 35.641727447509766\n",
      "After 25 steps: w is 4.968875408172607,    loss is 35.627471923828125\n",
      "After 26 steps: w is 4.967681407928467,    loss is 35.61322021484375\n",
      "After 27 steps: w is 4.966487884521484,    loss is 35.598976135253906\n",
      "After 28 steps: w is 4.965294361114502,    loss is 35.58473587036133\n",
      "After 29 steps: w is 4.964101314544678,    loss is 35.57050323486328\n",
      "After 30 steps: w is 4.9629082679748535,    loss is 35.5562744140625\n",
      "After 31 steps: w is 4.9617156982421875,    loss is 35.54205322265625\n",
      "After 32 steps: w is 4.9605231285095215,    loss is 35.527835845947266\n",
      "After 33 steps: w is 4.959331035614014,    loss is 35.51362609863281\n",
      "After 34 steps: w is 4.958138942718506,    loss is 35.499420166015625\n",
      "After 35 steps: w is 4.956947326660156,    loss is 35.48522186279297\n",
      "After 36 steps: w is 4.955755710601807,    loss is 35.47102737426758\n",
      "After 37 steps: w is 4.954564571380615,    loss is 35.45684051513672\n",
      "After 38 steps: w is 4.953373432159424,    loss is 35.44265365600586\n",
      "After 39 steps: w is 4.952182769775391,    loss is 35.4284782409668\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "\n",
    "loss = tf.square(w + 1)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print(\"After {} steps: w is {},    loss is {}\".format(i, w_val, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
